extern crate chrono;
extern crate cloud_storage;
extern crate log;
extern crate readability;
extern crate reqwest;
extern crate retry;
extern crate rss;
extern crate serde;

use chrono::{DateTime, NaiveDateTime, Utc};
use cloud_storage::Object;
use readability::extractor;
use retry::{delay::Exponential, retry};
use rss::{ChannelBuilder, ItemBuilder, validation::Validate};
use serde::Deserialize;
use std::env;

const HN_API_BASE: &str = "https://hacker-news.firebaseio.com/v0/";
const HN_ITEMS_LIMIT: u64 = 15;

const HN_URI_BASE: &str = "https://news.ycombinator.com/";
const SCRAPE_RETRY_INITIAL_DELAY: u64 = 1000;

const HN_CHANNEL_TITLE: &str = "Hacker News";
const HN_CHANNEL_LINK: &str = "https://news.ycombinator.com/";
const HN_CHANNEL_DESCRIPTION: &str = "Generated by RSSit. Hacker News is a social news website focusing on computer science and entrepreneurship.";
const HN_CLOUD_STORAGE_FILENAME: &str = "hackernews.rss";

const CLOUD_STORAGE_BUCKET: &str = "rssit.say11.com";

#[tokio::main]
async fn main() {
    env_logger::init();

    let request_url = String::from(format!(r#"{api_base}topstories.json?orderBy="$key"&limitToFirst={limit}"#, api_base=HN_API_BASE, limit=HN_ITEMS_LIMIT));
    let response = reqwest::get(&request_url).await.unwrap();
    let item_ids: Vec<u64> = response.json().await.unwrap();
    let mut items: Vec<rss::Item> = Vec::with_capacity(item_ids.len());
    for id in item_ids.iter() {
        let request_url = String::from(format!("{api_base}item/{id}.json", api_base=HN_API_BASE, id=id));
        let response = reqwest::get(&request_url).await.unwrap();
        let hn_item: HNItem = response.json().await.unwrap();
        let item_uri = format!("{uri_base}item?id={id}", uri_base=HN_URI_BASE, id=hn_item.id);
        let mut item = ItemBuilder::default();
        item.title(hn_item.title);
        let description = match &hn_item.url {
            Some(url) => match scrape_description(url) {
                Ok(t) => t.content,
                Err(e) => format!("Could not retrieve preview: {}", e)
            },
            None => match hn_item.text {
                Some(t) => t,
                None => String::from("")
            }
        };
        item.description(description);
        item.link(hn_item.url);
        item.comments(String::from(&item_uri));
        item.guid(rss::Guid{
            value: String::from(&item_uri),
            permalink: true
        });
        item.pub_date(DateTime::<Utc>::from_utc(NaiveDateTime::from_timestamp(hn_item.time, 0), Utc).to_rfc2822());
        let item = item.build().unwrap();
        items.push(item);
    }

    let mut channel = ChannelBuilder::default();
    channel.title(HN_CHANNEL_TITLE);
    channel.link(HN_CHANNEL_LINK);
    channel.description(HN_CHANNEL_DESCRIPTION);
    channel.items(items);
    let channel = channel.build().unwrap();
    channel.validate().unwrap();
    let mut buf = vec![];
    channel.pretty_write_to(&mut buf, b' ', 2).unwrap(); // // write the channel to a writer

    // if defined, use CLOUD_STORAGE_BUCKET environment variable as bucket name
    let bucket = match env::var("CLOUD_STORAGE_BUCKET") {
        Ok(b) => b,
        Err(_) => String::from(CLOUD_STORAGE_BUCKET)
    };
    Object::create(&*bucket, buf, HN_CLOUD_STORAGE_FILENAME, "application/rss+xml").await.unwrap();
}

fn scrape_description(url: &String) -> Result<readability::extractor::Product, retry::Error<readability::error::Error>> {
    retry(Exponential::from_millis(SCRAPE_RETRY_INITIAL_DELAY).take(3), || { extractor::scrape(&url) })
}

#[cfg(test)]
mod tests {
    use super::main;

    #[test]
    fn it_works() {
        main();
    }
}

#[derive(Deserialize, Debug)]
struct HNItem {
    id: u64,
    deleted: Option<bool>,
    r#type: String,
    by: String,
    time: i64,
    text: Option<String>,
    dead: Option<bool>,
    url: Option<String>,
    score: i64,
    title: String
}